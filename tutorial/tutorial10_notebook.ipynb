{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Course - Tutorial 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Tidy Data\n",
    "\n",
    "#### Overview: Wickham's Tidy Data Framework\n",
    "Hadley Wickham's seminal paper on **Tidy Data** focuses on structuring datasets to facilitate analysis. The principles are:\n",
    "1. **Each variable forms a column.**\n",
    "2. **Each observation forms a row.**\n",
    "3. **Each type of observational unit forms a table.**\n",
    "\n",
    "Tidy datasets ensure a consistent structure, simplifying manipulation, visualization, and modeling. However, real-world datasets often deviate from this structure, requiring transformation.\n",
    "\n",
    "#### Common Problems in Untidy Data\n",
    "Wickham identifies five types of untidy data:\n",
    "1. **Column headers are values, not variable names.**\n",
    "2. **Multiple variables are stored in one column.**\n",
    "3. **Variables are stored in both rows and columns.**\n",
    "4. **Multiple types of observational units are stored in the same table.**\n",
    "5. **One type of observational unit is spread out over multiple tables or files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Column Headers Are Values, Not Variable Names - Part 1\n",
    "\n",
    "In the Pew dataset column headers represent income brackets rather than variable names. Our goal is to restructure the data into a tidy format, with columns for `religion`, `income`, and `frequency`.\n",
    "\n",
    "#### Tasks:\n",
    "1. Load the Pew dataset from a CSV file.\n",
    "2. Inspect the data to identify untidy elements.\n",
    "3. Use Pandas' `melt` function to:\n",
    "   - Transform the income brackets (column headers) into a new column named `income`.\n",
    "   - Ensure the original `religion` column remains intact.\n",
    "   - Create a new column named `frequency` to hold the corresponding values.\n",
    "4. Sort the resulting tidy dataset by `religion` (alphabetically) and `frequency` (numerically).\n",
    "\n",
    "\n",
    "#### Dataset Preview:\n",
    "\n",
    "The untidy data might look like this:\n",
    "\n",
    "| religion           | \\<$10k | \\$10–20k | \\$20–30k | \\$30–40k | \\$40–50k | \\$50–75k |\n",
    "|--------------------|--------|---------|---------|---------|---------|---------|\n",
    "| Agnostic           | 27     | 34      | 60      | 81      | 76      | 137     |\n",
    "| Atheist            | 12     | 27      | 37      | 52      | 35      | 70      |\n",
    "| Buddhist           | 27     | 21      | 30      | 34      | 33      | 58      |\n",
    "| Catholic           | 418    | 617     | 732     | 670     | 638     | 1116    |\n",
    "| Don’t know/refused | 15     | 14      | 15      | 11      | 10      | 35      |\n",
    "\n",
    "Using `pd.melt`, we can achieve the following tidy dataset:\n",
    "\n",
    "| religion           | income   | frequency |\n",
    "|--------------------|----------|-----------|\n",
    "| Agnostic           | \\<\\$10k  | 27        |\n",
    "| Agnostic           | \\$10–20k | 34        |\n",
    "| Agnostic           | \\$20–30k | 60        |\n",
    "| Atheist            | \\<\\$10k  | 12        |\n",
    "| Atheist            | \\$10–20k | 27        |\n",
    "| Buddhist           | \\<\\$10k  | 27        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pew = pd.read_csv(\"data/pew_sample.csv\")\n",
    "\n",
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Column Headers Are Values, Not Variable Names - Part 2\n",
    "\n",
    "The **Billboard dataset** is stored in a wide format. The data contains weekly rankings (`wk1`, `wk2`, ..., `wk75`) spread across columns. Your task is to transform it into a **long format** where each row corresponds to a specific song's ranking in a particular week.\n",
    "\n",
    "#### Tasks:\n",
    "1. **Inspect the Dataset**: Familiarize yourself with the structure of the Billboard dataset. Identify columns to retain (e.g., `year`, `artist`, `track`, `time`, `date.entered`) and those to melt (e.g., `wk1`, `wk2`, ..., `wk75`).\n",
    "2. **Use `pd.melt`**:\n",
    "   - Reshape the dataset so that all weekly ranking columns (`wk1`, `wk2`, ..., `wk75`) are combined into a single `week` column.\n",
    "   - Create a new column `rank` to hold the corresponding ranking values.\n",
    "3. **Clean Up the Week Column**:\n",
    "   - Use string methods to extract only the numeric part of the week values (e.g., `wk1` → `1`).\n",
    "   - Convert the `week` column into an integer type for easier manipulation.\n",
    "\n",
    "**Hint:** Apply the `.str.replace()` or `.str.extract()` methods to clean up the `week` column.\n",
    "\n",
    "#### Dataset Preview:\n",
    "**Original Untidy Data (Wide Format)**:\n",
    "\n",
    "| year | artist       | track                    | time | date.entered | wk1 | wk2 | wk3 | ... | wk75 |\n",
    "|------|--------------|--------------------------|------|--------------|-----|-----|-----|-----|------|\n",
    "| 2000 | 2 Pac        | Baby Don’t Cry           | 4:22 | 2000-02-26   | 87  | 82  | 72  | ... | NaN  |\n",
    "| 2000 | 2Ge+her      | The Hardest Part Of ...  | 3:15 | 2000-09-02   | 91  | 87  | 82  | ... | NaN  |\n",
    "| 2000 | 3 Doors Down | Kryptonite               | 3:53 | 2000-04-08   | 81  | 70  | 66  | ... | NaN  |\n",
    "\n",
    "\n",
    "**After Transformation (Long Format)**:\n",
    "\n",
    "| year | artist       | time | track                  | date       | week | rank |\n",
    "|------|--------------|------|------------------------|------------|------|------|\n",
    "| 2000 | 2 Pac        | 4:22 | Baby Don’t Cry         | 2000-02-26 | 1    | 87   |\n",
    "| 2000 | 2 Pac        | 4:22 | Baby Don’t Cry         | 2000-03-04 | 2    | 82   |\n",
    "| 2000 | 2 Pac        | 4:22 | Baby Don’t Cry         | 2000-03-11 | 3    | 72   |\n",
    "| 2000 | 2Ge+her      | 3:15 | The Hardest Part Of ...| 2000-09-02 | 1    | 91   |\n",
    "| 2000 | 2Ge+her      | 3:15 | The Hardest Part Of ...| 2000-09-09 | 2    | 87   |\n",
    "| 2000 | 2Ge+her      | 3:15 | The Hardest Part Of ...| 2000-09-02 | 3    | 92   |\n",
    "| 2000 | 3 Doors Down | 3:53 | Kryptonite             | 2000-04-08 | 1    | 81   |\n",
    "| 2000 | 3 Doors Down | 3:53 | Kryptonite             | 2000-04-15 | 2    | 70   |\n",
    "| 2000 | 3 Doors Down | 3:53 | Kryptonite             | 2000-04-22 | 3    | 66   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "billboard = pd.read_csv(\"data/billboard.csv\")\n",
    "\n",
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Multiple Variables Are Stored in One Column\n",
    "\n",
    "In the tuberculosis (TB) dataset multiple variables (e.g., sex and age group) are combined into a single column. Your goal is to separate these into distinct columns, making the dataset tidy.\n",
    "\n",
    "\n",
    "#### Tasks:\n",
    "1. **Reshape the Dataset**: Use `pd.melt` to convert the wide format into a long format where the demographic variables (e.g., `m014`, `f1524`) are stored in a single column.\n",
    "2. **Split the Combined Column**: Extract the `sex` and `age` variables from the demographic column.\n",
    "3. **Clean the Age Values**: Use the `map` method with a dictionary to convert the age codes (e.g., `014`, `1524`) into human-readable ranges (e.g., `0–14`, `15–24`).\n",
    "\n",
    "\n",
    "#### Dataset Preview:\n",
    "**Original Untidy Data**:\n",
    "\n",
    "| country | year | m014 | m1524 | m2534 | m3544 | m4554 | m5564 | m65 | mu  | f014 | f1524 | f2534 |\n",
    "|---------|------|------|-------|-------|-------|-------|-------|-----|-----|------|-------|-------|\n",
    "| AD      | 2000 | 0    | 0     | 1     | 0     | 0     | 0     | 0   | --- | ---  | ---   | ---   |\n",
    "| AE      | 2000 | 2    | 4     | 4     | 6     | 5     | 12    | 10  | --- | 3    | 6     | 5     |\n",
    "| AF      | 2000 | 52   | 228   | 183   | 149   | 129   | 94    | 80  | --- | 93   | 142   | 128   |\n",
    "\n",
    "**After Transformation (Tidy Format)**:\n",
    "\n",
    "| country | year | sex | age  | cases |\n",
    "|---------|------|-----|------|-------|\n",
    "| AD      | 2000 | m   | 0–14 | 0     |\n",
    "| AE      | 2000 | m   | 0–14 | 2     |\n",
    "| AF      | 2000 | m   | 0–14 | 52    |\n",
    "| AD      | 2000 | f   | 15–24| 0     |\n",
    "| AE      | 2000 | f   | 15–24| 6     |\n",
    "| AF      | 2000 | f   | 15–24| 142   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = pd.read_csv(\"data/tb_sample.csv\")\n",
    "\n",
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Variables Are Stored in Both Rows and Columns (Bonus!)\n",
    "\n",
    "The weather dataset contains variables such as minimum and maximum temperature (`tmin`, `tmax`) that stored across both rows and columns. Your task is to tidy the dataset by ensuring each variable is represented in its own column.\n",
    "\n",
    "\n",
    "\n",
    "#### Tasks:\n",
    "1. **Reshape the Dataset**:\n",
    "   - Use `pd.melt` to gather all day columns (`d1`, `d2`, ..., `d31`) into a single column, converting the wide format into a long format.\n",
    "   - Create a new `date` column by combining `year`, `month`, and the day extracted from the melted column.\n",
    "\n",
    "2. **Separate Variables**:\n",
    "   - Pivot the data so that `tmin` and `tmax` are stored as separate columns, with each row representing a unique `date`.\n",
    "\n",
    "3. **Handle Missing Values**:\n",
    "   - Remove rows where the `value` is missing (`—`).\n",
    "\n",
    "**Hints:**\n",
    "- Combine `year`, `month`, and `day` into a single `date` column using `pd.to_datetime` or string formatting.\n",
    "- Use `pivot` or `unstack` to move the `element` values (`tmin`, `tmax`) into separate columns.\n",
    "\n",
    "#### Dataset Preview\n",
    "**Original Untidy Data**:\n",
    "\n",
    "| id      | year | month | element | d1   | d2   | d3   | ... | d8  |\n",
    "|---------|------|-------|---------|------|------|------|-----|------|\n",
    "| MX17004 | 2010 | 1     | tmax    | —    | —    | —    | ... | —    |\n",
    "| MX17004 | 2010 | 1     | tmin    | —    | —    | —    | ... | —    |\n",
    "| MX17004 | 2010 | 2     | tmax    | —    | 27.3 | 24.1 | ... | —    |\n",
    "| MX17004 | 2010 | 2     | tmin    | —    | 14.4 | 14.4 | ... | —    |\n",
    "\n",
    "**After Transformation (Tidy Format)**:\n",
    "\n",
    "| id      | date       | tmax  | tmin  |\n",
    "|---------|------------|-------|-------|\n",
    "| MX17004 | 2010-02-02 | 27.3  | 14.4  |\n",
    "| MX17004 | 2010-02-03 | 24.1  | 14.4  |\n",
    "| MX17004 | 2010-03-05 | 32.1  | 14.2  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4: Multiple Types of Observational Units Are Stored in the Same Table\n",
    "\n",
    "The Billboard dataset, contains information about tracks (e.g., artist, track title, and time) as well as their rankings over time. These represent two distinct types of observational units. Your goal is to split the dataset into two separate tables:\n",
    "1. One table for track information.\n",
    "2. Another table for weekly rankings.\n",
    "\n",
    "\n",
    "#### Steps to Complete:\n",
    "\n",
    "1. **Extract Track Information**: Identify the unique combinations of `artist`, `track`, and `time` and assign each unique combination a unique `id`.\n",
    "2. **Separate Weekly Rankings**: Create a new table for rankings, using the `id` from the track table to link the two datasets.\n",
    "3. **Join Tables When Needed**: Use the `merge` function to link track information with ranking data when required for analysis.\n",
    "\n",
    "**Hint:** Use `pd.drop_duplicates` to extract unique rows for the track table.\n",
    "\n",
    "\n",
    "#### Dataset Preview\n",
    "**Original Untidy Data** (Mixed Observational Units):\n",
    "\n",
    "| year | artist          | track              | time | date       | week | rank |\n",
    "|------|-----------------|--------------------|------|------------|------|------|\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-04-29 | 1    | 100  |\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-05-06 | 2    | 99   |\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-05-13 | 3    | 96   |\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-05-20 | 4    | 76   |\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-05-27 | 5    | 55   |\n",
    "\n",
    "\n",
    "**After Transformation: Two Separate Tables**\n",
    "\n",
    "1. **Track Table** (One row per unique track):\n",
    "\n",
    "| id | artist          | track              | time |\n",
    "|----|-----------------|--------------------|------|\n",
    "| 1  | Nelly           | Country Grammar    | 4:17 |\n",
    "\n",
    "2. **Rank Table** (One row per weekly ranking):\n",
    "\n",
    "| id | date       | rank |\n",
    "|----|------------|------|\n",
    "| 1  | 2000-04-29 | 100  |\n",
    "| 1  | 2000-05-06 | 99   |\n",
    "| 1  | 2000-05-13 | 96   |\n",
    "| 1  | 2000-05-20 | 76   |\n",
    "| 1  | 2000-05-27 | 55   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the billboard dataframe from exercise 1.2, after transforming it into a long format.\n",
    "\n",
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Visualizing the Palmer Penguins Dataset with Seaborn\n",
    "\n",
    "The Palmer Penguins dataset provides information about three penguin species—Adélie, Chinstrap, and Gentoo—observed in Antarctica. This dataset contains details such as body mass, flipper length, and bill dimensions, making it ideal for data visualization practice.\n",
    "\n",
    "#### Dataset Overview\n",
    "The dataset includes the following key columns:\n",
    "- `species`: The penguin species (Adélie, Chinstrap, Gentoo).\n",
    "- `island`: The island where the penguins were observed.\n",
    "- `bill_length_mm`: Length of the penguin's bill (in millimeters).\n",
    "- `bill_depth_mm`: Depth of the penguin's bill (in millimeters).\n",
    "- `flipper_length_mm`: Length of the penguin's flipper (in millimeters).\n",
    "- `body_mass_g`: Body mass of the penguin (in grams).\n",
    "- `sex`: The penguin's sex (male, female).\n",
    "\n",
    "The dataset can be loaded directly from the seaborn repository using the URL with `pd.read_csv(url)`:  \n",
    "<https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv>\n",
    "\n",
    "In the previous tutorial, we briefly explored how to create plots using **matplotlib**. Today, we will focus on **seaborn**, a powerful Python library built on top of matplotlib that allows us to create elegant and informative visualizations with minimal effort. By using seaborn, we can generate complex and aesthetically pleasing plots with just a few lines of code.\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. **Visualize the Distribution of Flipper Length**:\n",
    "   - Create a histogram or kernel density plot (KDE) of `flipper_length_mm`, with the distributions differentiated by `species` using the `hue` parameter.\n",
    "\n",
    "2. **Explore the Relationship Between Body Mass and Flipper Length**:\n",
    "   - Use a scatter plot to visualize the relationship between `body_mass_g` and `flipper_length_mm`.\n",
    "   - Differentiate the points by `species` using the `hue` parameter and use distinct markers for each species.\n",
    "\n",
    "3. **Compare Body Mass Across Species**:\n",
    "   - Create a boxplot to compare the distributions of `body_mass_g` across the three penguin species.\n",
    "\n",
    "4. **Analyze Pairwise Relationships**:\n",
    "   - Generate a pair plot for the numerical variables (`bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`), with the points colored by `species`. Use a KDE plot for the diagonal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Analyzing Stock Correlations and Volatility with Yahoo Finance Data\n",
    "\n",
    "With thousands of stocks being traded daily, making informed decisions about portfolio composition requires extracting meaningful insights from stock data. One useful strategy involves identifying **low-correlation stocks** to minimize portfolio risk, as combining such stocks helps diversify the portfolio and reduce its overall volatility.\n",
    "\n",
    "In this exercise, you will:\n",
    "1. **Download stock data** using the `yfinance` library for a list of tickers.\n",
    "2. **Calculate daily returns and correlations** between stock pairs.\n",
    "3. **Determine the two stocks with the lowest correlation** for potential use in hedging strategies.\n",
    "\n",
    "#### Why Low-Correlation Stocks?\n",
    "When combining two stocks with low or negative correlation:\n",
    "- The **expected return** of the portfolio is unaffected by the correlation and is given by:\n",
    "  \n",
    "  $$\n",
    "  \\mu_p = w_1 \\mu_1 + w_2 \\mu_2\n",
    "  $$\n",
    "  \n",
    "  where:\n",
    "  - $ w_1, w_2 $: Portfolio weights for the two stocks.\n",
    "  - $ \\mu_1, \\mu_2 $: Expected returns of the two stocks.\n",
    "\n",
    "- The **volatility** of the portfolio, however, is reduced and depends on the correlation ($ \\rho $):\n",
    "\n",
    "  $$\n",
    "  \\sigma_p = \\sqrt{w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2 + 2 w_1 w_2 \\sigma_1 \\sigma_2 \\rho}\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "  - $ \\sigma_1, \\sigma_2 $: Volatilities of the two stocks.\n",
    "  - $ \\rho $: Correlation between the two stocks.\n",
    "\n",
    "This formula highlights that lowering the correlation ($ \\rho $) reduces the portfolio's overall volatility ($ \\sigma_p $), making it an essential consideration for diversification and risk reduction.\n",
    "\n",
    "\n",
    "\n",
    "#### Tasks\n",
    "1. **Read the list of stock tickers**:\n",
    "   - Read a text file (`sp500-ticker-list.txt`) containing a list of stock tickers.\n",
    "\n",
    "2. **Download stock data**:\n",
    "   - Use the `yfinance` library to fetch historical data from 2000 and 2024 for each stock.\n",
    "   - Save the data to individual CSV files in a `stock_data/` directory.\n",
    "\n",
    "3. **Calculate daily returns**:\n",
    "   - Compute the daily returns as the percentage change of adjusted closing prices for each stock.\n",
    "\n",
    "4. **Calculate the correlation matrix**:\n",
    "   - Use NumPy to compute the correlation matrix of the daily returns.\n",
    "\n",
    "5. **Find the two least correlated stocks**:\n",
    "   - Identify the two stocks with the lowest correlation from the matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
