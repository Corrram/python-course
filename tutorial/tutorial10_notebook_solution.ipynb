{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Course - Tutorial 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Tidy Data\n",
    "\n",
    "#### Overview: Wickham's Tidy Data Framework\n",
    "Hadley Wickham's seminal paper on **Tidy Data** focuses on structuring datasets to facilitate analysis. The principles are:\n",
    "1. **Each variable forms a column.**\n",
    "2. **Each observation forms a row.**\n",
    "3. **Each type of observational unit forms a table.**\n",
    "\n",
    "Tidy datasets ensure a consistent structure, simplifying manipulation, visualization, and modeling. However, real-world datasets often deviate from this structure, requiring transformation.\n",
    "\n",
    "#### Common Problems in Untidy Data\n",
    "Wickham identifies four main types of untidy data:\n",
    "1. **Column headers are values, not variable names.**\n",
    "2. **Multiple variables are stored in one column.**\n",
    "3. **Variables are stored in both rows and columns.**\n",
    "4. **Multiple types of observational units are stored in the same table.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Column Headers Are Values, Not Variable Names - Part 1\n",
    "\n",
    "In the Pew dataset column headers represent income brackets rather than variable names. Our goal is to restructure the data into a tidy format, with columns for `religion`, `income`, and `frequency`.\n",
    "\n",
    "#### Tasks:\n",
    "1. Load the Pew dataset from a CSV file.\n",
    "2. Inspect the data to identify untidy elements.\n",
    "3. Use Pandas' `melt` function to:\n",
    "   - Transform the income brackets (column headers) into a new column named `income`.\n",
    "   - Ensure the original `religion` column remains intact.\n",
    "   - Create a new column named `frequency` to hold the corresponding values.\n",
    "4. Sort the resulting tidy dataset by `religion` (alphabetically) and `income` (numerically).\n",
    "\n",
    "\n",
    "#### Dataset Preview:\n",
    "\n",
    "The untidy data might look like this:\n",
    "\n",
    "| religion           | <$10k | $10–20k | $20–30k | $30–40k | $40–50k | $50–75k |\n",
    "|--------------------|--------|---------|---------|---------|---------|---------|\n",
    "| Agnostic           | 27     | 34      | 60      | 81      | 76      | 137     |\n",
    "| Atheist            | 12     | 27      | 37      | 52      | 35      | 70      |\n",
    "| Buddhist           | 27     | 21      | 30      | 34      | 33      | 58      |\n",
    "| Catholic           | 418    | 617     | 732     | 670     | 638     | 1116    |\n",
    "| Don’t know/refused | 15     | 14      | 15      | 11      | 10      | 35      |\n",
    "\n",
    "Using `pd.melt`, we can achieve the following tidy dataset:\n",
    "\n",
    "| religion           | income    | frequency |\n",
    "|--------------------|-----------|-----------|\n",
    "| Agnostic           | <$10k     | 27        |\n",
    "| Agnostic           | $10–20k   | 34        |\n",
    "| Agnostic           | $20–30k   | 60        |\n",
    "| Atheist            | <$10k     | 12        |\n",
    "| Atheist            | $10–20k   | 27        |\n",
    "| Buddhist           | <$10k     | 27        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Pew sample dataset\n",
    "pew_data = pd.read_csv(\"data/pew_sample.csv\")\n",
    "\n",
    "# Reshape the Pew dataset from wide format to long format\n",
    "pew_long_format = pd.melt(\n",
    "    frame=pew_data,\n",
    "    id_vars=[\"religion\"],  # Keep the \"religion\" column fixed\n",
    "    value_vars=pew_data.columns[1:],  # Use all other columns as value variables\n",
    "    var_name=\"income_bracket\",  \n",
    "    value_name=\"frequency\",  \n",
    ")\n",
    "\n",
    "# Sort the reshaped data by religion and income_bracket\n",
    "pew_long_sorted = pew_long_format.sort_values(by=[\"religion\", \"income_bracket\"])\n",
    "\n",
    "# Display the first rows of sorted data\n",
    "pew_long_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Column Headers Are Values, Not Variable Names - Part 2\n",
    "\n",
    "The **Billboard dataset** is stored in a wide format. The data contains weekly rankings (`wk1`, `wk2`, ..., `wk75`) spread across columns. Your task is to transform it into a **long format** where each row corresponds to a specific song's ranking in a particular week.\n",
    "\n",
    "#### Tasks:\n",
    "1. **Inspect the Dataset**: Familiarize yourself with the structure of the Billboard dataset. Identify columns to retain (e.g., `year`, `artist`, `track`, `time`, `date.entered`) and those to melt (e.g., `wk1`, `wk2`, ..., `wk75`).\n",
    "2. **Use `pd.melt`**:\n",
    "   - Reshape the dataset so that all weekly ranking columns (`wk1`, `wk2`, ..., `wk75`) are combined into a single `week` column.\n",
    "   - Create a new column `rank` to hold the corresponding ranking values.\n",
    "3. **Clean Up the Week Column**:\n",
    "   - Use string methods to extract only the numeric part of the week values (e.g., `wk1` → `1`).\n",
    "   - Convert the `week` column into an integer type for easier manipulation.\n",
    "\n",
    "**Hint:** Apply the `.str.replace()` or `.str.extract()` methods to clean up the `week` column.\n",
    "\n",
    "#### Dataset Preview:\n",
    "**Original Untidy Data (Wide Format)**:\n",
    "\n",
    "| year | artist       | track                    | time | date.entered | wk1 | wk2 | wk3 | ... | wk75 |\n",
    "|------|--------------|--------------------------|------|--------------|-----|-----|-----|-----|------|\n",
    "| 2000 | 2 Pac        | Baby Don’t Cry           | 4:22 | 2000-02-26   | 87  | 82  | 72  | ... | NaN  |\n",
    "| 2000 | 2Ge+her      | The Hardest Part Of ...  | 3:15 | 2000-09-02   | 91  | 87  | 82  | ... | NaN  |\n",
    "| 2000 | 3 Doors Down | Kryptonite               | 3:53 | 2000-04-08   | 81  | 70  | 66  | ... | NaN  |\n",
    "\n",
    "\n",
    "**After Transformation (Long Format)**:\n",
    "\n",
    "| year | artist       | time | track                  | date       | week | rank |\n",
    "|------|--------------|------|------------------------|------------|------|------|\n",
    "| 2000 | 2 Pac        | 4:22 | Baby Don’t Cry         | 2000-02-26 | 1    | 87   |\n",
    "| 2000 | 2 Pac        | 4:22 | Baby Don’t Cry         | 2000-03-04 | 2    | 82   |\n",
    "| 2000 | 2 Pac        | 4:22 | Baby Don’t Cry         | 2000-03-11 | 3    | 72   |\n",
    "| 2000 | 2Ge+her      | 3:15 | The Hardest Part Of ...| 2000-09-02 | 1    | 91   |\n",
    "| 2000 | 2Ge+her      | 3:15 | The Hardest Part Of ...| 2000-09-09 | 2    | 87   |\n",
    "| 2000 | 2Ge+her      | 3:15 | The Hardest Part Of ...| 2000-09-02 | 3    | 92   |\n",
    "| 2000 | 3 Doors Down | 3:53 | Kryptonite             | 2000-04-08 | 1    | 81   |\n",
    "| 2000 | 3 Doors Down | 3:53 | Kryptonite             | 2000-04-15 | 2    | 70   |\n",
    "| 2000 | 3 Doors Down | 3:53 | Kryptonite             | 2000-04-22 | 3    | 66   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Billboard dataset\n",
    "billboard_data = pd.read_csv(\"data/billboard.csv\")\n",
    "\n",
    "# Define columns to keep fixed during the transformation\n",
    "id_columns = [\n",
    "    \"year\",\n",
    "    \"artist.inverted\",\n",
    "    \"track\",\n",
    "    \"time\",\n",
    "    \"genre\",\n",
    "    \"date.entered\",\n",
    "    \"date.peaked\",\n",
    "]\n",
    "\n",
    "# Reshape the dataset from wide to long format\n",
    "# Columns not in id_columns will become the 'week' column in the resulting DataFrame\n",
    "billboard_long_format = pd.melt(\n",
    "    frame=billboard_data,\n",
    "    id_vars=id_columns,\n",
    "    var_name=\"week\",  # Name for the variable column (week number column)\n",
    "    value_name=\"rank\",  # Name for the value column (chart rank)\n",
    ")\n",
    "\n",
    "# Clean the 'week' column by removing unwanted text patterns and convert to integer\n",
    "billboard_long_format[\"week\"] = billboard_long_format[\"week\"].str.replace(\n",
    "    \"x|st.week|th.week|nd.week|rd.week\", \"\", regex=True\n",
    ").astype(int)\n",
    "\n",
    "# Calculate the actual chart date by adding the week offset to the 'date.entered'\n",
    "billboard_long_format[\"date\"] = pd.to_datetime(billboard_long_format[\"date.entered\"]) + pd.to_timedelta(\n",
    "    (billboard_long_format[\"week\"] - 1) * 7, \"d\"\n",
    ")\n",
    "\n",
    "# Sort the dataset by track and date for better readability\n",
    "billboard_long_sorted = billboard_long_format.sort_values(by=[\"track\", \"date\"])\n",
    "\n",
    "# Drop rows with missing values (e.g., ranks not recorded for certain weeks)\n",
    "billboard_cleaned = billboard_long_sorted.dropna()\n",
    "\n",
    "# Display the cleaned and processed DataFrame\n",
    "billboard_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Multiple Variables Are Stored in One Column\n",
    "\n",
    "In the tuberculosis (TB) dataset multiple variables (e.g., sex and age group) are combined into a single column. Your goal is to separate these into distinct columns, making the dataset tidy.\n",
    "\n",
    "\n",
    "#### Tasks:\n",
    "1. **Reshape the Dataset**: Use `pd.melt` to convert the wide format into a long format where the demographic variables (e.g., `m014`, `f1524`) are stored in a single column.\n",
    "2. **Split the Combined Column**: Extract the `sex` and `age` variables from the demographic column.\n",
    "3. **Clean the Age Values**: Use the `map` method with a dictionary to convert the age codes (e.g., `014`, `1524`) into human-readable ranges (e.g., `0–14`, `15–24`).\n",
    "\n",
    "\n",
    "#### Dataset Preview:\n",
    "**Original Untidy Data**:\n",
    "\n",
    "| country | year | m014 | m1524 | m2534 | m3544 | m4554 | m5564 | m65 | mu  | f014 | f1524 | f2534 |\n",
    "|---------|------|------|-------|-------|-------|-------|-------|-----|-----|------|-------|-------|\n",
    "| AD      | 2000 | 0    | 0     | 1     | 0     | 0     | 0     | 0   | --- | ---  | ---   | ---   |\n",
    "| AE      | 2000 | 2    | 4     | 4     | 6     | 5     | 12    | 10  | --- | 3    | 6     | 5     |\n",
    "| AF      | 2000 | 52   | 228   | 183   | 149   | 129   | 94    | 80  | --- | 93   | 142   | 128   |\n",
    "\n",
    "**After Transformation (Tidy Format)**:\n",
    "\n",
    "| country | year | sex | age  | cases |\n",
    "|---------|------|-----|------|-------|\n",
    "| AD      | 2000 | m   | 0–14 | 0     |\n",
    "| AE      | 2000 | m   | 0–14 | 2     |\n",
    "| AF      | 2000 | m   | 0–14 | 52    |\n",
    "| AD      | 2000 | f   | 15–24| 0     |\n",
    "| AE      | 2000 | f   | 15–24| 6     |\n",
    "| AF      | 2000 | f   | 15–24| 142   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TB dataset\n",
    "tb_data = pd.read_csv(\"data/tb_sample.csv\")\n",
    "\n",
    "# Reshape the dataset from wide to long format\n",
    "tb_long_format = pd.melt(\n",
    "    tb_data,\n",
    "    id_vars=[\"country\", \"year\"],  # Keep \"country\" and \"year\" as fixed identifiers\n",
    "    value_vars=list(tb_data.columns)[2:],  # Columns to unpivot\n",
    "    var_name=\"column\",  \n",
    "    value_name=\"cases\",  \n",
    ")\n",
    "\n",
    "# Extract the 'sex' information from the 'column' field\n",
    "tb_long_format[\"sex\"] = tb_long_format[\"column\"].str[0]\n",
    "\n",
    "# Extract the 'age' information and map it to readable age groups\n",
    "tb_long_format[\"age\"] = tb_long_format[\"column\"].str[1:].map(\n",
    "    {\n",
    "        \"014\": \"0-14\",\n",
    "        \"1524\": \"15-24\",\n",
    "        \"2534\": \"25-34\",\n",
    "        \"3544\": \"35-44\",\n",
    "        \"4554\": \"45-54\",\n",
    "        \"5564\": \"55-64\",\n",
    "        \"65\": \"65+\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Drop unnecessary columns after extracting useful information\n",
    "tb_cleaned = tb_long_format.drop(columns=[\"column\"])\n",
    "\n",
    "# Display the cleaned and formatted tb dataset\n",
    "tb_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Variables Are Stored in Both Rows and Columns (Bonus!)\n",
    "\n",
    "The weather dataset contains variables such as minimum and maximum temperature (`tmin`, `tmax`) that stored across both rows and columns. Your task is to tidy the dataset by ensuring each variable is represented in its own column.\n",
    "\n",
    "\n",
    "\n",
    "#### Tasks:\n",
    "1. **Reshape the Dataset**:\n",
    "   - Use `pd.melt` to gather all day columns (`d1`, `d2`, ..., `d31`) into a single column, converting the wide format into a long format.\n",
    "   - Create a new `date` column by combining `year`, `month`, and the day extracted from the melted column.\n",
    "\n",
    "2. **Separate Variables**:\n",
    "   - Pivot the data so that `tmin` and `tmax` are stored as separate columns, with each row representing a unique `date`.\n",
    "\n",
    "3. **Handle Missing Values**:\n",
    "   - Remove rows where the `value` is missing (`—`).\n",
    "\n",
    "**Hints:**\n",
    "- Combine `year`, `month`, and `day` into a single `date` column using `pd.to_datetime` or string formatting.\n",
    "- Use `pivot` or `unstack` to move the `element` values (`tmin`, `tmax`) into separate columns.\n",
    "\n",
    "#### Dataset Preview\n",
    "**Original Untidy Data**:\n",
    "\n",
    "| id      | year | month | element | d1   | d2   | d3   | ... | d8  |\n",
    "|---------|------|-------|---------|------|------|------|-----|------|\n",
    "| MX17004 | 2010 | 1     | tmax    | —    | —    | —    | ... | —    |\n",
    "| MX17004 | 2010 | 1     | tmin    | —    | —    | —    | ... | —    |\n",
    "| MX17004 | 2010 | 2     | tmax    | —    | 27.3 | 24.1 | ... | —    |\n",
    "| MX17004 | 2010 | 2     | tmin    | —    | 14.4 | 14.4 | ... | —    |\n",
    "\n",
    "**After Transformation (Tidy Format)**:\n",
    "\n",
    "| id      | date       | tmax  | tmin  |\n",
    "|---------|------------|-------|-------|\n",
    "| MX17004 | 2010-02-02 | 27.3  | 14.4  |\n",
    "| MX17004 | 2010-02-03 | 24.1  | 14.4  |\n",
    "| MX17004 | 2010-03-05 | 32.1  | 14.2  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the weather sample dataset\n",
    "weather_data = pd.read_csv(\"data/weather_sample.csv\")\n",
    "\n",
    "# Transform the data from wide format to long format\n",
    "weather_long = pd.melt(\n",
    "    frame=weather_data,\n",
    "    id_vars=[\"id\", \"year\", \"month\", \"element\"],\n",
    "    var_name=\"day\",\n",
    "    value_name=\"value\",\n",
    ")\n",
    "\n",
    "# Remove the leading \"d\" from the day column and convert to integer\n",
    "weather_long[\"day\"] = weather_long[\"day\"].str[1:].astype(\"int\")\n",
    "\n",
    "# Combine year, month, and day into a single date column in YYYY-MM-DD format\n",
    "weather_long[\"date\"] = weather_long[[\"year\", \"month\", \"day\"]].apply(\n",
    "    lambda row: \"{:4d}-{:02d}-{:02d}\".format(*row), axis=1\n",
    ")\n",
    "\n",
    "# Filter out rows with missing values and keep relevant columns\n",
    "weather_filtered = weather_long.loc[\n",
    "    ~weather_long[\"value\"].isna(), [\"id\", \"date\", \"element\", \"value\"]\n",
    "]\n",
    "\n",
    "# Set index to a multi-index of id, date, and element\n",
    "weather_indexed = weather_filtered.set_index([\"id\", \"date\", \"element\"])\n",
    "\n",
    "# Unstack the element level of the index to create separate columns for each element\n",
    "weather_wide = weather_indexed.unstack()\n",
    "\n",
    "# Flatten the multi-index column names\n",
    "weather_wide.columns = list(weather_wide.columns.get_level_values(\"element\"))\n",
    "\n",
    "# Reset the index to convert it back into a DataFrame\n",
    "weather_cleaned = weather_wide.reset_index()\n",
    "\n",
    "# Display the formatted dataframe\n",
    "weather_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4: Multiple Types of Observational Units Are Stored in the Same Table\n",
    "\n",
    "The Billboard dataset, contains information about tracks (e.g., artist, track title, and time) as well as their rankings over time. These represent two distinct types of observational units. Your goal is to split the dataset into two separate tables:\n",
    "1. One table for track information.\n",
    "2. Another table for weekly rankings.\n",
    "\n",
    "\n",
    "#### Steps to Complete:\n",
    "\n",
    "1. **Extract Track Information**: Identify the unique combinations of `artist`, `track`, and `time` and assign each unique combination a unique `id`.\n",
    "2. **Separate Weekly Rankings**: Create a new table for rankings, using the `id` from the track table to link the two datasets.\n",
    "3. **Join Tables When Needed**: Use the `merge` function to link track information with ranking data when required for analysis.\n",
    "\n",
    "**Hint:** Use `pd.drop_duplicates` to extract unique rows for the track table.\n",
    "\n",
    "\n",
    "#### Dataset Preview\n",
    "**Original Untidy Data** (Mixed Observational Units):\n",
    "\n",
    "| year | artist          | track              | time | date       | week | rank |\n",
    "|------|-----------------|--------------------|------|------------|------|------|\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-04-29 | 1    | 100  |\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-05-06 | 2    | 99   |\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-05-13 | 3    | 96   |\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-05-20 | 4    | 76   |\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-05-27 | 5    | 55   |\n",
    "\n",
    "\n",
    "**After Transformation: Two Separate Tables**\n",
    "\n",
    "1. **Track Table** (One row per unique track):\n",
    "\n",
    "| id | artist          | track              | time |\n",
    "|----|-----------------|--------------------|------|\n",
    "| 1  | Nelly           | Country Grammar    | 4:17 |\n",
    "\n",
    "2. **Rank Table** (One row per weekly ranking):\n",
    "\n",
    "| id | date       | rank |\n",
    "|----|------------|------|\n",
    "| 1  | 2000-04-29 | 100  |\n",
    "| 1  | 2000-05-06 | 99   |\n",
    "| 1  | 2000-05-13 | 96   |\n",
    "| 1  | 2000-05-20 | 76   |\n",
    "| 1  | 2000-05-27 | 55   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the billboard DataFrame from exercise 1.2 and ensure it is in long format.\n",
    "\n",
    "# Extract unique tracks along with their artist and duration\n",
    "unique_tracks = billboard_cleaned[[\"artist.inverted\", \"track\", \"time\"]].drop_duplicates()\n",
    "\n",
    "# Assign a unique ID to each track\n",
    "unique_tracks.insert(0, \"track_id\", range(1, len(unique_tracks) + 1))\n",
    "\n",
    "# Merge the original DataFrame with the unique tracks, adding track IDs to each record\n",
    "billboard_with_track_id = pd.merge(billboard_cleaned, unique_tracks, on=[\"artist.inverted\", \"track\", \"time\"])\n",
    "\n",
    "# Retain only the relevant columns: track ID, date, and rank\n",
    "billboard_with_track_id = billboard_with_track_id[[\"track_id\", \"date\", \"rank\"]]\n",
    "\n",
    "# Merge data for a specific artist (e.g., \"Nelly\") with track details\n",
    "pd.merge(\n",
    "    unique_tracks[unique_tracks[\"artist.inverted\"] == \"Nelly\"],\n",
    "    billboard_with_track_id,\n",
    "    on=[\"track_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Visualizing the Palmer Penguins Dataset with Seaborn\n",
    "\n",
    "The Palmer Penguins dataset provides information about three penguin species—Adélie, Chinstrap, and Gentoo—observed in Antarctica. This dataset contains details such as body mass, flipper length, and bill dimensions, making it ideal for data visualization practice.\n",
    "\n",
    "#### Dataset Overview\n",
    "The dataset includes the following key columns:\n",
    "- `species`: The penguin species (Adélie, Chinstrap, Gentoo).\n",
    "- `island`: The island where the penguins were observed.\n",
    "- `bill_length_mm`: Length of the penguin's bill (in millimeters).\n",
    "- `bill_depth_mm`: Depth of the penguin's bill (in millimeters).\n",
    "- `flipper_length_mm`: Length of the penguin's flipper (in millimeters).\n",
    "- `body_mass_g`: Body mass of the penguin (in grams).\n",
    "- `sex`: The penguin's sex (male, female).\n",
    "\n",
    "The dataset can be loaded directly from the seaborn repository using the URL with `pd.read_csv(url)`:  \n",
    "`https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv`\n",
    "\n",
    "In the previous tutorial, we briefly explored how to create plots using **matplotlib**. Today, we will focus on **seaborn**, a powerful Python library built on top of matplotlib that allows us to create elegant and informative visualizations with minimal effort. By using seaborn, we can generate complex and aesthetically pleasing plots with just a few lines of code.\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. **Visualize the Distribution of Flipper Length**:\n",
    "   - Create a histogram or kernel density plot (KDE) of `flipper_length_mm`, with the distributions differentiated by `species` using the `hue` parameter.\n",
    "\n",
    "2. **Explore the Relationship Between Body Mass and Flipper Length**:\n",
    "   - Use a scatter plot to visualize the relationship between `body_mass_g` and `flipper_length_mm`.\n",
    "   - Differentiate the points by `species` using the `hue` parameter and use distinct markers for each species.\n",
    "\n",
    "3. **Compare Body Mass Across Species**:\n",
    "   - Create a boxplot to compare the distributions of `body_mass_g` across the three penguin species.\n",
    "\n",
    "4. **Analyze Pairwise Relationships**:\n",
    "   - Generate a pair plot for the numerical variables (`bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`), with the points colored by `species`. Use a KDE plot for the diagonal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Palmer Penguins dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
    "penguins = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "display(penguins.head())\n",
    "\n",
    "# 1. Distribution of Flipper Length\n",
    "sns.histplot(\n",
    "    data=penguins, \n",
    "    x=\"flipper_length_mm\", \n",
    "    hue=\"species\", \n",
    "    kde=True, \n",
    "    palette=\"pastel\",\n",
    "    bins=20\n",
    ")\n",
    "plt.title(\"Distribution of Flipper Length by Species\")\n",
    "plt.xlabel(\"Flipper Length (mm)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Relationship Between Body Mass and Flipper Length\n",
    "sns.scatterplot(\n",
    "    data=penguins, \n",
    "    x=\"flipper_length_mm\", \n",
    "    y=\"body_mass_g\", \n",
    "    hue=\"species\", \n",
    "    style=\"species\", \n",
    "    palette=\"Set2\"\n",
    ")\n",
    "plt.title(\"Body Mass vs. Flipper Length by Species\")\n",
    "plt.xlabel(\"Flipper Length (mm)\")\n",
    "plt.ylabel(\"Body Mass (g)\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Comparing Body Mass Across Species\n",
    "sns.boxplot(\n",
    "    data=penguins, \n",
    "    x=\"species\", \n",
    "    y=\"body_mass_g\", \n",
    "    palette=\"viridis\", \n",
    "    hue=\"species\"\n",
    ")\n",
    "plt.title(\"Body Mass Distribution Across Species\")\n",
    "plt.xlabel(\"Species\")\n",
    "plt.ylabel(\"Body Mass (g)\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Pair Plot of Numerical Variables\n",
    "sns.pairplot(\n",
    "    data=penguins, \n",
    "    hue=\"species\", \n",
    "    palette=\"husl\", \n",
    "    diag_kind=\"kde\", \n",
    "    markers=[\"o\", \"s\", \"D\"]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Analyzing Stock Correlations and Volatility with Yahoo Finance Data\n",
    "\n",
    "With thousands of stocks being traded daily, making informed decisions about portfolio composition requires extracting meaningful insights from stock data. One useful strategy involves identifying **low-correlation stocks** to minimize portfolio risk, as combining such stocks helps diversify the portfolio and reduce its overall volatility.\n",
    "\n",
    "In this exercise, you will:\n",
    "1. **Download stock data** using the `yfinance` library for a list of tickers.\n",
    "2. **Calculate daily returns and correlations** between stock pairs.\n",
    "3. **Determine the two stocks with the lowest correlation** for potential use in hedging strategies.\n",
    "\n",
    "#### Why Low-Correlation Stocks?\n",
    "When combining two stocks with low or negative correlation:\n",
    "- The **expected return** of the portfolio is unaffected by the correlation and is given by:\n",
    "  \n",
    "  $$\n",
    "  \\mu_p = w_1 \\mu_1 + w_2 \\mu_2\n",
    "  $$\n",
    "  \n",
    "  where:\n",
    "  - $ w_1, w_2 $: Portfolio weights for the two stocks.\n",
    "  - $ \\mu_1, \\mu_2 $: Expected returns of the two stocks.\n",
    "\n",
    "- The **volatility** of the portfolio, however, is reduced and depends on the correlation ($ \\rho $):\n",
    "\n",
    "  $$\n",
    "  \\sigma_p = \\sqrt{w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2 + 2 w_1 w_2 \\sigma_1 \\sigma_2 \\rho}\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "  - $ \\sigma_1, \\sigma_2 $: Volatilities of the two stocks.\n",
    "  - $ \\rho $: Correlation between the two stocks.\n",
    "\n",
    "This formula highlights that lowering the correlation ($ \\rho $) reduces the portfolio's overall volatility ($ \\sigma_p $), making it an essential consideration for diversification and risk reduction.\n",
    "\n",
    "\n",
    "\n",
    "#### Tasks\n",
    "1. **Read the list of stock tickers**:\n",
    "   - Read a text file (`sp500-ticker-list.txt`) containing a list of stock tickers.\n",
    "\n",
    "2. **Download stock data**:\n",
    "   - Use the `yfinance` library to fetch historical data from 2000 and 2024 for each stock.\n",
    "   - Save the data to individual CSV files in a `stock_data/` directory.\n",
    "\n",
    "3. **Calculate daily returns**:\n",
    "   - Compute the daily returns as the percentage change of adjusted closing prices for each stock.\n",
    "\n",
    "4. **Calculate the correlation matrix**:\n",
    "   - Use NumPy to compute the correlation matrix of the daily returns.\n",
    "\n",
    "5. **Find the two least correlated stocks**:\n",
    "   - Identify the two stocks with the lowest correlation from the matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Step 1: Read the list of stock tickers from a file\n",
    "with open(\"data/sp500-ticker-list.txt\") as f:\n",
    "    stock_list = f.read().splitlines()\n",
    "\n",
    "# Step 2: Create a directory for the data\n",
    "pathlib.Path(\"stock_data\").mkdir(exist_ok=True)\n",
    "\n",
    "# Step 3: Download stock data and save to CSV files\n",
    "for stock in stock_list:\n",
    "    data = yf.download(stock, start=\"2000-01-01\", end=\"2024-12-31\", interval=\"1d\")\n",
    "    data.columns = data.columns.get_level_values(0)\n",
    "    file_name = f\"stock_data/{stock}.csv\"\n",
    "    data.to_csv(file_name)\n",
    "    # print(f\"{stock} data saved to {file_name}\")\n",
    "\n",
    "# Step 4: Load stock data and calculate daily returns\n",
    "missing_data = []\n",
    "stock_data = {}\n",
    "for file in glob.glob(\"stock_data/*.csv\"):\n",
    "    stock_name = pathlib.Path(file).stem\n",
    "    df = pd.read_csv(file)\n",
    "    try:\n",
    "        df[\"Daily_Return\"] = df[\"Close\"].pct_change()  # Calculate daily returns\n",
    "    except:\n",
    "        missing_data.append(stock_name) # Append to missing_data list, if the df is empty\n",
    "        continue\n",
    "    df.dropna(subset=[\"Daily_Return\"], inplace=True)  # Drop rows with NaN daily returns\n",
    "    stock_data[stock_name] = df[\"Daily_Return\"].values\n",
    "\n",
    "# Step 5: Filter stocks with complete data\n",
    "max_data_len = max(len(data) for data in stock_data.values())\n",
    "stock_data = {name: data for name, data in stock_data.items() if len(data) == max_data_len}\n",
    "\n",
    "# Step 6: Compute the correlation matrix\n",
    "correlation_matrix = np.corrcoef(list(stock_data.values()))\n",
    "\n",
    "# Step 7: Find the two stocks with the smallest correlation\n",
    "i, j = np.unravel_index(\n",
    "    np.argmin(correlation_matrix),\n",
    "    correlation_matrix.shape\n",
    ")\n",
    "stock_names = list(stock_data.keys())\n",
    "stock1, stock2 = stock_names[i], stock_names[j]\n",
    "\n",
    "print(f\"The two stocks with the lowest correlation are: {stock1} and {stock2}\")\n",
    "print(f\"Excluded stocks (due to missing data) are: {missing_data}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
